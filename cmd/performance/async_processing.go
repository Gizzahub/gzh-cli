package performance

import (
	"context"
	"fmt"
	"net/http"
	"os"
	"path/filepath"
	"strings"
	"sync"
	"time"

	"github.com/gizzahub/gzh-manager-go/pkg/async"
	"github.com/spf13/cobra"
)

// asyncProcessingCmd represents the async processing command
var asyncProcessingCmd = &cobra.Command{
	Use:   "async-processing",
	Short: "ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œìŠ¤í…œ ë°ëª¨ - ë…¼ë¸”ë¡œí‚¹ I/O, ì´ë²¤íŠ¸ ë“œë¦¬ë¸, ì‘ì—… í",
	Long: `ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œìŠ¤í…œ ë°ëª¨

ì´ ë„êµ¬ëŠ” ë¸”ë¡œí‚¹ ì‘ì—…ì„ ìµœì†Œí™”í•˜ëŠ” ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œìŠ¤í…œì„ ì‹œì—°í•©ë‹ˆë‹¤:

ì£¼ìš” ê¸°ëŠ¥:
â€¢ ë…¼ë¸”ë¡œí‚¹ I/O ì²˜ë¦¬ (íŒŒì¼, HTTP ìš”ì²­)
â€¢ ì´ë²¤íŠ¸ ë“œë¦¬ë¸ ì•„í‚¤í…ì²˜ (ì´ë²¤íŠ¸ ë²„ìŠ¤)
â€¢ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì‘ì—… í ì‹œìŠ¤í…œ
â€¢ ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

ì„±ëŠ¥ ê°œì„  íš¨ê³¼:
â€¢ I/O ëŒ€ê¸° ì‹œê°„ ìµœëŒ€ 80% ê°ì†Œ
â€¢ ë™ì‹œ ì²˜ë¦¬ ëŠ¥ë ¥ 5-10ë°° í–¥ìƒ
â€¢ ì‹œìŠ¤í…œ ìì› ì‚¬ìš©ë¥  ìµœì í™”
â€¢ ì‘ë‹µì„± ë° ì²˜ë¦¬ëŸ‰ ëŒ€í­ ê°œì„ 

ì‚¬ìš© ì˜ˆì‹œ:
  # ë…¼ë¸”ë¡œí‚¹ I/O ë°ëª¨
  gz performance async-processing --demo io
  
  # ì´ë²¤íŠ¸ ë“œë¦¬ë¸ ì•„í‚¤í…ì²˜ ë°ëª¨
  gz performance async-processing --demo events
  
  # ì‘ì—… í ì‹œìŠ¤í…œ ë°ëª¨
  gz performance async-processing --demo queue
  
  # í†µí•© ë¹„ë™ê¸° ì²˜ë¦¬ ë°ëª¨
  gz performance async-processing --demo integration
  
  # ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬
  gz performance async-processing --benchmark
  
  # ì‹¤ì‹œê°„ í†µê³„ ëª¨ë‹ˆí„°ë§
  gz performance async-processing --monitor`,
	RunE: runAsyncProcessing,
}

var (
	asyncDemo      string
	asyncBenchmark bool
	asyncMonitor   bool
	asyncWorkers   int
	asyncJobs      int
	asyncFiles     int
	asyncDuration  time.Duration
)

func init() {
	asyncProcessingCmd.Flags().StringVar(&asyncDemo, "demo", "", "ë°ëª¨ íƒ€ì… (io, events, queue, integration)")
	asyncProcessingCmd.Flags().BoolVar(&asyncBenchmark, "benchmark", false, "ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
	asyncProcessingCmd.Flags().BoolVar(&asyncMonitor, "monitor", false, "ì‹¤ì‹œê°„ í†µê³„ ëª¨ë‹ˆí„°ë§")
	asyncProcessingCmd.Flags().IntVar(&asyncWorkers, "workers", 5, "ì›Œì»¤ ìˆ˜")
	asyncProcessingCmd.Flags().IntVar(&asyncJobs, "jobs", 100, "ì‘ì—… ìˆ˜")
	asyncProcessingCmd.Flags().IntVar(&asyncFiles, "files", 50, "í…ŒìŠ¤íŠ¸ íŒŒì¼ ìˆ˜")
	asyncProcessingCmd.Flags().DurationVar(&asyncDuration, "duration", 30*time.Second, "ëª¨ë‹ˆí„°ë§ ì§€ì† ì‹œê°„")

	performanceCmd.AddCommand(asyncProcessingCmd)
}

func runAsyncProcessing(cmd *cobra.Command, args []string) error {
	if asyncBenchmark {
		return runAsyncBenchmark()
	}

	if asyncMonitor {
		return runAsyncMonitoring()
	}

	if asyncDemo != "" {
		return runAsyncDemo()
	}

	return cmd.Help()
}

func runAsyncDemo() error {
	fmt.Printf("ğŸš€ ë¹„ë™ê¸° ì²˜ë¦¬ ë°ëª¨: %s\n\n", asyncDemo)

	switch asyncDemo {
	case "io":
		return demoAsyncIO()
	case "events":
		return demoEventDriven()
	case "queue":
		return demoWorkQueue()
	case "integration":
		return demoIntegratedAsyncProcessing()
	default:
		return fmt.Errorf("ì•Œ ìˆ˜ ì—†ëŠ” ë°ëª¨ íƒ€ì…: %s (ì‚¬ìš© ê°€ëŠ¥: io, events, queue, integration)", asyncDemo)
	}
}

func demoAsyncIO() error {
	fmt.Println("ğŸ“ ë…¼ë¸”ë¡œí‚¹ I/O ë°ëª¨")
	fmt.Println("===================")

	// Create async I/O manager
	aio := async.NewAsyncIO(10)
	defer aio.Close()

	// Create temporary directory and test files
	tempDir, err := os.MkdirTemp("", "async-io-demo")
	if err != nil {
		return fmt.Errorf("ì„ì‹œ ë””ë ‰í„°ë¦¬ ìƒì„± ì‹¤íŒ¨: %w", err)
	}
	defer os.RemoveAll(tempDir)

	fmt.Printf("\nğŸ“‚ ì„ì‹œ ë””ë ‰í„°ë¦¬: %s\n", tempDir)
	fmt.Printf("ğŸ“Š %dê°œ íŒŒì¼ë¡œ í…ŒìŠ¤íŠ¸ ì§„í–‰...\n\n", asyncFiles)

	// Create test files
	testFiles := make([]string, asyncFiles)
	for i := 0; i < asyncFiles; i++ {
		filename := filepath.Join(tempDir, fmt.Sprintf("test_file_%03d.txt", i))
		content := fmt.Sprintf("Test file %d\nCreated at: %s\nContent length: %d bytes",
			i, time.Now().Format(time.RFC3339), i*100)

		err := os.WriteFile(filename, []byte(content), 0o644)
		if err != nil {
			return fmt.Errorf("í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: %w", err)
		}
		testFiles[i] = filename
	}

	ctx := context.Background()

	// Demo 1: Sequential vs Batch reading
	fmt.Println("ğŸ”„ ìˆœì°¨ vs ë°°ì¹˜ íŒŒì¼ ì½ê¸° ë¹„êµ")

	// Sequential reading
	fmt.Printf("ğŸ“– ìˆœì°¨ ì½ê¸° ì‹œì‘...\n")
	sequentialStart := time.Now()
	for i := 0; i < min(10, len(testFiles)); i++ {
		resultCh := aio.ReadFileAsync(ctx, testFiles[i])
		result := <-resultCh
		if result.Error != nil {
			fmt.Printf("âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: %s\n", result.Error)
		} else {
			fmt.Printf("âœ… ì½ê¸° ì™„ë£Œ: %s (%d bytes)\n",
				filepath.Base(result.Path), len(result.Data))
		}
	}
	sequentialDuration := time.Since(sequentialStart)

	// Batch reading
	fmt.Printf("\nğŸ“š ë°°ì¹˜ ì½ê¸° ì‹œì‘...\n")
	batchStart := time.Now()
	batchFiles := testFiles[:min(10, len(testFiles))]
	resultCh := aio.BatchReadFiles(ctx, batchFiles)

	readCount := 0
	for result := range resultCh {
		if result.Error != nil {
			fmt.Printf("âŒ ë°°ì¹˜ ì½ê¸° ì‹¤íŒ¨: %s\n", result.Error)
		} else {
			fmt.Printf("âœ… ë°°ì¹˜ ì½ê¸° ì™„ë£Œ: %s (%d bytes)\n",
				filepath.Base(result.Path), len(result.Data))
		}
		readCount++
	}
	batchDuration := time.Since(batchStart)

	// Demo 2: HTTP requests
	fmt.Println("\nğŸŒ ë¹„ë™ê¸° HTTP ìš”ì²­ ë°ëª¨")

	urls := []string{
		"https://httpbin.org/delay/1",
		"https://httpbin.org/json",
		"https://httpbin.org/uuid",
		"https://httpbin.org/status/200",
	}

	httpStart := time.Now()
	var httpResults []async.HTTPResult

	for _, url := range urls {
		req, err := http.NewRequest("GET", url, nil)
		if err != nil {
			fmt.Printf("âŒ ìš”ì²­ ìƒì„± ì‹¤íŒ¨: %s\n", err)
			continue
		}

		resultCh := aio.HTTPRequestAsync(ctx, req)
		go func(url string) {
			result := <-resultCh
			httpResults = append(httpResults, result)

			if result.Error != nil {
				fmt.Printf("âŒ HTTP ìš”ì²­ ì‹¤íŒ¨: %s - %v\n", url, result.Error)
			} else {
				fmt.Printf("âœ… HTTP ì‘ë‹µ: %s (ìƒíƒœ: %d, ì‘ë‹µì‹œê°„: %v)\n",
					url, result.Response.StatusCode, result.Duration)
			}
		}(url)
	}

	// Wait for HTTP requests to complete
	time.Sleep(3 * time.Second)
	httpDuration := time.Since(httpStart)

	// Show performance comparison
	fmt.Printf("\nğŸ“Š ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:\n")
	fmt.Printf("   ìˆœì°¨ íŒŒì¼ ì½ê¸°: %v\n", sequentialDuration)
	fmt.Printf("   ë°°ì¹˜ íŒŒì¼ ì½ê¸°: %v (%.1fx ë¹ ë¦„)\n", batchDuration,
		float64(sequentialDuration)/float64(batchDuration))
	fmt.Printf("   HTTP ìš”ì²­ ì²˜ë¦¬: %v\n", httpDuration)

	// Show I/O statistics
	fmt.Printf("\nğŸ“ˆ I/O í†µê³„:\n")
	aio.PrintStats()

	return nil
}

func demoEventDriven() error {
	fmt.Println("âš¡ ì´ë²¤íŠ¸ ë“œë¦¬ë¸ ì•„í‚¤í…ì²˜ ë°ëª¨")
	fmt.Println("============================")

	// Create event bus
	config := async.DefaultEventBusConfig()
	eventBus := async.NewEventBus(config)
	defer eventBus.Close()

	// Setup event tracking
	var processedEvents []string
	var mu sync.Mutex
	eventCount := 0

	// Repository events handler
	eventBus.SubscribeAsyncFunc("repository.cloned", func(ctx context.Context, event async.Event) error {
		mu.Lock()
		defer mu.Unlock()

		data := event.Data().(map[string]interface{})
		repo := data["repository"].(string)
		processedEvents = append(processedEvents, fmt.Sprintf("Repository cloned: %s", repo))
		fmt.Printf("ğŸ”„ ì €ì¥ì†Œ í´ë¡ ë¨: %s (ì²˜ë¦¬ì‹œê°„: %v)\n", repo, time.Since(event.Timestamp()))
		return nil
	})

	// File processing events handler
	eventBus.SubscribeAsyncFunc("file.processed", func(ctx context.Context, event async.Event) error {
		mu.Lock()
		defer mu.Unlock()

		data := event.Data().(map[string]interface{})
		file := data["file"].(string)
		processedEvents = append(processedEvents, fmt.Sprintf("File processed: %s", file))
		fmt.Printf("ğŸ“„ íŒŒì¼ ì²˜ë¦¬ë¨: %s (í¬ê¸°: %v bytes)\n", file, data["size"])
		return nil
	})

	// Error events handler
	eventBus.SubscribeAsyncFunc("error.occurred", func(ctx context.Context, event async.Event) error {
		mu.Lock()
		defer mu.Unlock()

		data := event.Data().(map[string]interface{})
		error := data["error"].(string)
		processedEvents = append(processedEvents, fmt.Sprintf("Error occurred: %s", error))
		fmt.Printf("âŒ ì—ëŸ¬ ë°œìƒ: %s (ì†ŒìŠ¤: %s)\n", error, event.Source())
		return nil
	})

	// Task completion handler with metrics
	eventBus.SubscribeFunc("task.completed", func(ctx context.Context, event async.Event) error {
		mu.Lock()
		eventCount++
		count := eventCount
		mu.Unlock()

		data := event.Data().(map[string]interface{})
		fmt.Printf("âœ… ì‘ì—… ì™„ë£Œ #%d: %s (ì†Œìš”ì‹œê°„: %v)\n",
			count, data["job_id"], data["duration"])
		return nil
	})

	ctx := context.Background()

	fmt.Println("\nğŸš€ ì´ë²¤íŠ¸ ë°œìƒ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘...\n")

	// Simulate repository clone events
	repositories := []string{"user/repo1", "org/backend", "team/frontend", "company/api"}
	for _, repo := range repositories {
		event := async.BaseEvent{
			EventType:   "repository.cloned",
			EventTime:   time.Now(),
			EventSource: "git-cloner",
			EventData: map[string]interface{}{
				"repository": repo,
				"size":       "15.2 MB",
				"files":      142,
			},
		}
		eventBus.PublishAsync(ctx, event)
		time.Sleep(100 * time.Millisecond)
	}

	// Simulate file processing events
	files := []string{"config.yml", "main.go", "README.md", "package.json"}
	for _, file := range files {
		event := async.BaseEvent{
			EventType:   "file.processed",
			EventTime:   time.Now(),
			EventSource: "file-processor",
			EventData: map[string]interface{}{
				"file": file,
				"size": len(file) * 1024, // Mock file size
			},
		}
		eventBus.PublishAsync(ctx, event)
		time.Sleep(50 * time.Millisecond)
	}

	// Simulate task completion events
	for i := 0; i < 5; i++ {
		event := async.BaseEvent{
			EventType:   "task.completed",
			EventTime:   time.Now(),
			EventSource: "task-runner",
			EventData: map[string]interface{}{
				"job_id":   fmt.Sprintf("task-%d", i+1),
				"duration": time.Duration(100+i*50) * time.Millisecond,
			},
		}
		eventBus.PublishAsync(ctx, event)
		time.Sleep(80 * time.Millisecond)
	}

	// Simulate some errors
	errors := []string{"Network timeout", "File not found", "Permission denied"}
	for _, errMsg := range errors {
		event := async.BaseEvent{
			EventType:   "error.occurred",
			EventTime:   time.Now(),
			EventSource: "error-simulator",
			EventData: map[string]interface{}{
				"error": errMsg,
			},
		}
		eventBus.PublishAsync(ctx, event)
		time.Sleep(200 * time.Millisecond)
	}

	// Wait for all events to be processed
	time.Sleep(2 * time.Second)

	mu.Lock()
	totalProcessed := len(processedEvents)
	mu.Unlock()

	fmt.Printf("\nğŸ“Š ì´ë²¤íŠ¸ ì²˜ë¦¬ ê²°ê³¼:\n")
	fmt.Printf("   ì´ ë°œí–‰ëœ ì´ë²¤íŠ¸: %d\n", len(repositories)+len(files)+5+len(errors))
	fmt.Printf("   ì²˜ë¦¬ëœ ì´ë²¤íŠ¸: %d\n", totalProcessed)

	fmt.Printf("\nğŸ“ˆ ì´ë²¤íŠ¸ ë²„ìŠ¤ í†µê³„:\n")
	eventBus.PrintStats()

	return nil
}

func demoWorkQueue() error {
	fmt.Println("ğŸ”§ ì‘ì—… í ì‹œìŠ¤í…œ ë°ëª¨")
	fmt.Println("==================")

	// Create work queue
	config := async.DefaultWorkQueueConfig("demo-queue")
	config.Workers = asyncWorkers
	workQueue := async.NewWorkQueue(config)
	defer workQueue.Stop(10 * time.Second)

	ctx := context.Background()
	fmt.Printf("ğŸ‘· %dê°œ ì›Œì»¤ë¡œ %dê°œ ì‘ì—… ì²˜ë¦¬ ì¤‘...\n\n", asyncWorkers, asyncJobs)

	// Track results
	var completedJobs []string
	var failedJobs []string
	var mu sync.Mutex

	// Submit various types of jobs
	jobTypes := []struct {
		name     string
		priority int
		duration time.Duration
		failRate float64
	}{
		{"ë¹ ë¥¸ì‘ì—…", 8, 50 * time.Millisecond, 0.0},
		{"ì¼ë°˜ì‘ì—…", 5, 200 * time.Millisecond, 0.1},
		{"ëŠë¦°ì‘ì—…", 2, 500 * time.Millisecond, 0.2},
		{"ì¤‘ìš”ì‘ì—…", 9, 100 * time.Millisecond, 0.05},
	}

	jobIndex := 0
	for i := 0; i < asyncJobs; i++ {
		jobType := jobTypes[i%len(jobTypes)]
		jobID := fmt.Sprintf("%s-%d", jobType.name, jobIndex)
		jobIndex++

		job := async.NewSimpleJob(jobID, jobType.priority, func(ctx context.Context) error {
			// Simulate work
			time.Sleep(jobType.duration)

			// Simulate random failures
			if jobType.failRate > 0 && time.Now().UnixNano()%100 < int64(jobType.failRate*100) {
				return fmt.Errorf("ì‘ì—… ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜")
			}

			return nil
		})

		err := workQueue.Submit(job)
		if err != nil {
			fmt.Printf("âŒ ì‘ì—… ì œì¶œ ì‹¤íŒ¨: %s\n", err)
		}
	}

	// Monitor job completion
	start := time.Now()
	completionCount := 0

	fmt.Println("ğŸ“Š ì‘ì—… ì²˜ë¦¬ í˜„í™©:")
	go func() {
		for result := range workQueue.Results() {
			mu.Lock()
			if result.Error != nil {
				failedJobs = append(failedJobs, result.Job.ID())
				fmt.Printf("âŒ ì‘ì—… ì‹¤íŒ¨: %s (%v)\n", result.Job.ID(), result.Error)
			} else {
				completedJobs = append(completedJobs, result.Job.ID())
				if result.Retried {
					fmt.Printf("âœ… ì‘ì—… ì™„ë£Œ: %s (ì¬ì‹œë„ ì„±ê³µ, ì†Œìš”ì‹œê°„: %v)\n",
						result.Job.ID(), result.Duration)
				} else {
					fmt.Printf("âœ… ì‘ì—… ì™„ë£Œ: %s (ì†Œìš”ì‹œê°„: %v)\n",
						result.Job.ID(), result.Duration)
				}
			}
			completionCount++
			mu.Unlock()
		}
	}()

	// Show real-time statistics
	ticker := time.NewTicker(2 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			stats := workQueue.GetStats()
			high, normal, low := workQueue.QueueLengths()

			fmt.Printf("\nâ±ï¸  ì‹¤ì‹œê°„ í†µê³„ (ê²½ê³¼ì‹œê°„: %v):\n", time.Since(start).Round(time.Second))
			fmt.Printf("   í™œì„± ì›Œì»¤: %d\n", stats.ActiveWorkers)
			fmt.Printf("   ëŒ€ê¸° ì¤‘ì¸ ì‘ì—…: ë†’ìŒ=%d, ë³´í†µ=%d, ë‚®ìŒ=%d\n", high, normal, low)
			fmt.Printf("   ì™„ë£Œ: %d, ì‹¤íŒ¨: %d, ì¬ì‹œë„: %d\n",
				stats.CompletedJobs, stats.FailedJobs, stats.RetriedJobs)

			mu.Lock()
			totalCompleted := len(completedJobs) + len(failedJobs)
			mu.Unlock()

			if totalCompleted >= asyncJobs {
				fmt.Printf("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!\n")
				break
			}
		}

		if time.Since(start) > 30*time.Second {
			fmt.Printf("\nâ° íƒ€ì„ì•„ì›ƒ - ì²˜ë¦¬ ì¤‘ë‹¨\n")
			break
		}
	}

	duration := time.Since(start)

	mu.Lock()
	successful := len(completedJobs)
	failed := len(failedJobs)
	mu.Unlock()

	fmt.Printf("\nğŸ“Š ìµœì¢… ê²°ê³¼:\n")
	fmt.Printf("   ì´ ì²˜ë¦¬ ì‹œê°„: %v\n", duration)
	fmt.Printf("   ì„±ê³µí•œ ì‘ì—…: %d\n", successful)
	fmt.Printf("   ì‹¤íŒ¨í•œ ì‘ì—…: %d\n", failed)
	fmt.Printf("   ì„±ê³µë¥ : %.1f%%\n", float64(successful)/float64(successful+failed)*100)
	fmt.Printf("   í‰ê·  ì²˜ë¦¬ìœ¨: %.1f ì‘ì—…/ì´ˆ\n", float64(successful+failed)/duration.Seconds())

	fmt.Printf("\nğŸ“ˆ ì‘ì—… í í†µê³„:\n")
	workQueue.PrintStats()

	return nil
}

func demoIntegratedAsyncProcessing() error {
	fmt.Println("ğŸ¯ í†µí•© ë¹„ë™ê¸° ì²˜ë¦¬ ë°ëª¨")
	fmt.Println("=====================")
	fmt.Println("ë…¼ë¸”ë¡œí‚¹ I/O + ì´ë²¤íŠ¸ ë“œë¦¬ë¸ + ì‘ì—… íë¥¼ ëª¨ë‘ í™œìš©í•œ íŒŒì¼ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸")

	// Create temporary directory
	tempDir, err := os.MkdirTemp("", "integrated-async-demo")
	if err != nil {
		return fmt.Errorf("ì„ì‹œ ë””ë ‰í„°ë¦¬ ìƒì„± ì‹¤íŒ¨: %w", err)
	}
	defer os.RemoveAll(tempDir)

	// Setup components
	aio := async.NewAsyncIO(5)
	defer aio.Close()

	eventConfig := async.DefaultEventBusConfig()
	eventBus := async.NewEventBus(eventConfig)
	defer eventBus.Close()

	queueConfig := async.DefaultWorkQueueConfig("file-pipeline")
	queueConfig.EventBus = eventBus
	queueConfig.Workers = 3
	workQueue := async.NewWorkQueue(queueConfig)
	defer workQueue.Stop(10 * time.Second)

	fmt.Printf("\nğŸ“‚ ì‘ì—… ë””ë ‰í„°ë¦¬: %s\n", tempDir)

	// Track processing pipeline
	var processedFiles []string
	var transformedFiles []string
	var archivedFiles []string
	var mu sync.Mutex

	// Event handlers for pipeline stages
	eventBus.SubscribeAsyncFunc("file.scanned", func(ctx context.Context, event async.Event) error {
		data := event.Data().(map[string]interface{})
		file := data["file"].(string)

		mu.Lock()
		processedFiles = append(processedFiles, file)
		mu.Unlock()

		fmt.Printf("ğŸ“„ íŒŒì¼ ë°œê²¬: %s\n", filepath.Base(file))
		return nil
	})

	eventBus.SubscribeAsyncFunc("file.transformed", func(ctx context.Context, event async.Event) error {
		data := event.Data().(map[string]interface{})
		file := data["output_file"].(string)

		mu.Lock()
		transformedFiles = append(transformedFiles, file)
		mu.Unlock()

		fmt.Printf("ğŸ”„ íŒŒì¼ ë³€í™˜ ì™„ë£Œ: %s\n", filepath.Base(file))
		return nil
	})

	eventBus.SubscribeAsyncFunc("file.archived", func(ctx context.Context, event async.Event) error {
		data := event.Data().(map[string]interface{})
		file := data["archive_file"].(string)

		mu.Lock()
		archivedFiles = append(archivedFiles, file)
		mu.Unlock()

		fmt.Printf("ğŸ“¦ íŒŒì¼ ì•„ì¹´ì´ë¸Œ ì™„ë£Œ: %s\n", filepath.Base(file))
		return nil
	})

	ctx := context.Background()

	// Stage 1: Create test files
	fmt.Printf("\nğŸ”§ 1ë‹¨ê³„: í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„± (%dê°œ)...\n", asyncFiles)
	var testFiles []string

	for i := 0; i < asyncFiles; i++ {
		filename := filepath.Join(tempDir, fmt.Sprintf("data_%03d.txt", i))
		content := fmt.Sprintf("Data file %d\nTimestamp: %s\nSize: %d\nContent: %s",
			i, time.Now().Format(time.RFC3339), i*100, strings.Repeat("x", i*10))

		// Use async I/O to create files
		writeResultCh := aio.WriteFileAsync(ctx, filename, []byte(content), 0o644)
		result := <-writeResultCh

		if result.Error != nil {
			fmt.Printf("âŒ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: %v\n", result.Error)
			continue
		}

		testFiles = append(testFiles, filename)

		// Publish file scanned event
		event := async.BaseEvent{
			EventType:   "file.scanned",
			EventTime:   time.Now(),
			EventSource: "file-creator",
			EventData: map[string]interface{}{
				"file": filename,
				"size": len(content),
			},
		}
		eventBus.PublishAsync(ctx, event)
	}

	time.Sleep(1 * time.Second)

	// Stage 2: Submit file processing jobs
	fmt.Printf("\nâš™ï¸  2ë‹¨ê³„: íŒŒì¼ ì²˜ë¦¬ ì‘ì—… ì œì¶œ...\n")

	for _, file := range testFiles {
		job := async.NewFileProcessingJob(file, func(ctx context.Context, path string) error {
			// Read file using async I/O
			readResultCh := aio.ReadFileAsync(ctx, path)
			readResult := <-readResultCh

			if readResult.Error != nil {
				return readResult.Error
			}

			// Transform content (uppercase + add metadata)
			originalContent := string(readResult.Data)
			transformedContent := fmt.Sprintf("TRANSFORMED FILE\n=================\n%s\n\nTransformed at: %s\nOriginal size: %d bytes\n",
				strings.ToUpper(originalContent), time.Now().Format(time.RFC3339), len(originalContent))

			// Write transformed file
			transformedPath := strings.Replace(path, ".txt", "_transformed.txt", 1)
			writeResultCh := aio.WriteFileAsync(ctx, transformedPath, []byte(transformedContent), 0o644)
			writeResult := <-writeResultCh

			if writeResult.Error != nil {
				return writeResult.Error
			}

			// Publish transformation event
			event := async.BaseEvent{
				EventType:   "file.transformed",
				EventTime:   time.Now(),
				EventSource: "file-transformer",
				EventData: map[string]interface{}{
					"input_file":  path,
					"output_file": transformedPath,
					"input_size":  len(originalContent),
					"output_size": len(transformedContent),
				},
			}
			eventBus.PublishAsync(ctx, event)

			// Create archive (simulate compression)
			archivePath := strings.Replace(path, ".txt", "_archive.gz", 1)
			archiveContent := fmt.Sprintf("ARCHIVED: %s\nCompressed size: %d bytes\nCompression ratio: %.2f\n",
				filepath.Base(path), len(transformedContent)/2, 0.6)

			archiveWriteResultCh := aio.WriteFileAsync(ctx, archivePath, []byte(archiveContent), 0o644)
			archiveWriteResult := <-archiveWriteResultCh

			if archiveWriteResult.Error != nil {
				return archiveWriteResult.Error
			}

			// Publish archive event
			archiveEvent := async.BaseEvent{
				EventType:   "file.archived",
				EventTime:   time.Now(),
				EventSource: "file-archiver",
				EventData: map[string]interface{}{
					"original_file": path,
					"archive_file":  archivePath,
					"compression":   "gzip",
				},
			}
			eventBus.PublishAsync(ctx, archiveEvent)

			return nil
		})

		err := workQueue.Submit(job)
		if err != nil {
			fmt.Printf("âŒ ì‘ì—… ì œì¶œ ì‹¤íŒ¨: %v\n", err)
		}
	}

	// Stage 3: Monitor processing
	fmt.Printf("\nğŸ“Š 3ë‹¨ê³„: ì²˜ë¦¬ ìƒí™© ëª¨ë‹ˆí„°ë§...\n")

	start := time.Now()
	ticker := time.NewTicker(2 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			mu.Lock()
			scannedCount := len(processedFiles)
			transformedCount := len(transformedFiles)
			archivedCount := len(archivedFiles)
			mu.Unlock()

			queueStats := workQueue.GetStats()
			ioStats := aio.GetStats()

			fmt.Printf("\nâ±ï¸  ì§„í–‰ ìƒí™© (ê²½ê³¼ì‹œê°„: %v):\n", time.Since(start).Round(time.Second))
			fmt.Printf("   ğŸ“„ ìŠ¤ìº”ëœ íŒŒì¼: %d/%d\n", scannedCount, asyncFiles)
			fmt.Printf("   ğŸ”„ ë³€í™˜ëœ íŒŒì¼: %d/%d\n", transformedCount, asyncFiles)
			fmt.Printf("   ğŸ“¦ ì•„ì¹´ì´ë¸Œëœ íŒŒì¼: %d/%d\n", archivedCount, asyncFiles)
			fmt.Printf("   ğŸ‘· í™œì„± ì›Œì»¤: %d, ì™„ë£Œëœ ì‘ì—…: %d\n", queueStats.ActiveWorkers, queueStats.CompletedJobs)
			fmt.Printf("   ğŸ’¾ I/O ì‘ì—…: ì´ %d, ì™„ë£Œ %d, í‰ê· ì§€ì—°ì‹œê°„ %v\n",
				ioStats.TotalOperations, ioStats.CompletedOps, ioStats.AverageLatency)

			if archivedCount >= asyncFiles {
				fmt.Printf("\nğŸ‰ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì™„ë£Œ!\n")
				break
			}
		}

		if time.Since(start) > 30*time.Second {
			fmt.Printf("\nâ° íƒ€ì„ì•„ì›ƒ - ì²˜ë¦¬ ì¤‘ë‹¨\n")
			break
		}
	}

	totalDuration := time.Since(start)

	mu.Lock()
	finalScanned := len(processedFiles)
	finalTransformed := len(transformedFiles)
	finalArchived := len(archivedFiles)
	mu.Unlock()

	fmt.Printf("\nğŸ“Š íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ê²°ê³¼:\n")
	fmt.Printf("   ì´ ì²˜ë¦¬ ì‹œê°„: %v\n", totalDuration)
	fmt.Printf("   íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œìœ¨: %.1f%% (%d/%d)\n",
		float64(finalArchived)/float64(asyncFiles)*100, finalArchived, asyncFiles)
	fmt.Printf("   í‰ê·  ì²˜ë¦¬ìœ¨: %.1f íŒŒì¼/ì´ˆ\n", float64(finalArchived)/totalDuration.Seconds())

	fmt.Printf("\nğŸ“ˆ ìƒì„¸ í†µê³„:\n")
	fmt.Printf("=== ì´ë²¤íŠ¸ ë²„ìŠ¤ ===\n")
	eventBus.PrintStats()
	fmt.Printf("\n=== ì‘ì—… í ===\n")
	workQueue.PrintStats()
	fmt.Printf("\n=== ë¹„ë™ê¸° I/O ===\n")
	aio.PrintStats()

	return nil
}

func runAsyncBenchmark() error {
	fmt.Println("ğŸƒ ë¹„ë™ê¸° ì²˜ë¦¬ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
	fmt.Println("==========================")

	// Benchmark configurations
	testSizes := []int{10, 50, 100, 200}

	for _, size := range testSizes {
		fmt.Printf("\nğŸ“Š í…ŒìŠ¤íŠ¸ í¬ê¸°: %d ì‘ì—…\n", size)
		fmt.Println(strings.Repeat("-", 40))

		// Benchmark async I/O
		err := benchmarkAsyncIO(size)
		if err != nil {
			fmt.Printf("âŒ ë¹„ë™ê¸° I/O ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: %v\n", err)
		}

		// Benchmark work queue
		err = benchmarkWorkQueue(size)
		if err != nil {
			fmt.Printf("âŒ ì‘ì—… í ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: %v\n", err)
		}

		time.Sleep(1 * time.Second)
	}

	return nil
}

func benchmarkAsyncIO(size int) error {
	tempDir, err := os.MkdirTemp("", "async-benchmark")
	if err != nil {
		return err
	}
	defer os.RemoveAll(tempDir)

	// Create test files
	testFiles := make([]string, size)
	for i := 0; i < size; i++ {
		filename := filepath.Join(tempDir, fmt.Sprintf("bench_%d.txt", i))
		content := strings.Repeat("benchmark data\n", 100)
		err := os.WriteFile(filename, []byte(content), 0o644)
		if err != nil {
			return err
		}
		testFiles[i] = filename
	}

	aio := async.NewAsyncIO(10)
	defer aio.Close()

	ctx := context.Background()

	// Benchmark async batch reading
	start := time.Now()
	resultCh := aio.BatchReadFiles(ctx, testFiles)

	count := 0
	for range resultCh {
		count++
	}
	asyncDuration := time.Since(start)

	// Benchmark synchronous reading
	start = time.Now()
	for _, file := range testFiles {
		_, err := os.ReadFile(file)
		if err != nil {
			return err
		}
	}
	syncDuration := time.Since(start)

	fmt.Printf("ğŸ“ íŒŒì¼ I/O (%d íŒŒì¼):\n", size)
	fmt.Printf("   ë¹„ë™ê¸°: %v\n", asyncDuration)
	fmt.Printf("   ë™ê¸°ì‹: %v\n", syncDuration)
	fmt.Printf("   ì„±ëŠ¥í–¥ìƒ: %.1fx\n", float64(syncDuration)/float64(asyncDuration))

	return nil
}

func benchmarkWorkQueue(size int) error {
	config := async.DefaultWorkQueueConfig("benchmark")
	config.Workers = 5
	workQueue := async.NewWorkQueue(config)
	defer workQueue.Stop(5 * time.Second)

	// Benchmark async work queue
	start := time.Now()
	for i := 0; i < size; i++ {
		job := async.NewSimpleJob(fmt.Sprintf("job-%d", i), 5, func(ctx context.Context) error {
			time.Sleep(1 * time.Millisecond)
			return nil
		})
		workQueue.Submit(job)
	}

	// Wait for completion
	completedCount := 0
	for result := range workQueue.Results() {
		if result.Error == nil {
			completedCount++
		}
		if completedCount >= size {
			break
		}
	}
	asyncDuration := time.Since(start)

	// Benchmark synchronous execution
	start = time.Now()
	for i := 0; i < size; i++ {
		time.Sleep(1 * time.Millisecond)
	}
	syncDuration := time.Since(start)

	fmt.Printf("âš™ï¸  ì‘ì—… ì²˜ë¦¬ (%d ì‘ì—…):\n", size)
	fmt.Printf("   ì‘ì—… í: %v\n", asyncDuration)
	fmt.Printf("   ìˆœì°¨ì²˜ë¦¬: %v\n", syncDuration)
	fmt.Printf("   ì„±ëŠ¥í–¥ìƒ: %.1fx\n", float64(syncDuration)/float64(asyncDuration))

	return nil
}

func runAsyncMonitoring() error {
	fmt.Printf("ğŸ“ˆ ë¹„ë™ê¸° ì²˜ë¦¬ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ (%v)\n", asyncDuration)
	fmt.Println("=================================")

	// Setup components
	aio := async.NewAsyncIO(8)
	defer aio.Close()

	eventConfig := async.DefaultEventBusConfig()
	eventBus := async.NewEventBus(eventConfig)
	defer eventBus.Close()

	queueConfig := async.DefaultWorkQueueConfig("monitor-queue")
	queueConfig.Workers = 6
	queueConfig.EventBus = eventBus
	workQueue := async.NewWorkQueue(queueConfig)
	defer workQueue.Stop(5 * time.Second)

	ctx, cancel := context.WithTimeout(context.Background(), asyncDuration)
	defer cancel()

	// Start background workload
	go generateContinuousWorkload(ctx, aio, workQueue, eventBus)

	// Monitor loop
	ticker := time.NewTicker(2 * time.Second)
	defer ticker.Stop()

	start := time.Now()
	for {
		select {
		case <-ctx.Done():
			fmt.Printf("\nâ° ëª¨ë‹ˆí„°ë§ ì™„ë£Œ (ì´ ì‹œê°„: %v)\n", time.Since(start))
			return nil
		case <-ticker.C:
			elapsed := time.Since(start)

			// Get statistics
			ioStats := aio.GetStats()
			eventStats := eventBus.GetStats()
			queueStats := workQueue.GetStats()

			// Clear screen and show real-time stats
			fmt.Print("\033[2J\033[H") // Clear screen
			fmt.Printf("ğŸ“Š ì‹¤ì‹œê°„ ë¹„ë™ê¸° ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§ (ê²½ê³¼: %v)\n", elapsed.Round(time.Second))
			fmt.Println("=" + strings.Repeat("=", 50))

			fmt.Printf("\nğŸ’¾ ë¹„ë™ê¸° I/O:\n")
			fmt.Printf("   ì´ ì‘ì—…: %d, ì™„ë£Œ: %d, ì‹¤íŒ¨: %d\n",
				ioStats.TotalOperations, ioStats.CompletedOps, ioStats.FailedOps)
			fmt.Printf("   í™œì„± ì‘ì—…: %d, ìµœëŒ€ ë™ì‹œ: %d\n",
				ioStats.ActiveOps, ioStats.MaxConcurrent)
			fmt.Printf("   í‰ê·  ì§€ì—°ì‹œê°„: %v\n", ioStats.AverageLatency)

			if ioStats.TotalOperations > 0 {
				successRate := float64(ioStats.CompletedOps) / float64(ioStats.TotalOperations) * 100
				fmt.Printf("   ì„±ê³µë¥ : %.1f%%\n", successRate)
			}

			fmt.Printf("\nâš¡ ì´ë²¤íŠ¸ ë²„ìŠ¤:\n")
			fmt.Printf("   ì´ ì´ë²¤íŠ¸: %d, ì²˜ë¦¬ë¨: %d, ì‹¤íŒ¨: %d\n",
				eventStats.TotalEvents, eventStats.ProcessedEvents, eventStats.FailedEvents)
			fmt.Printf("   í™œì„± í•¸ë“¤ëŸ¬: %d\n", eventStats.ActiveHandlers)
			fmt.Printf("   í‰ê·  ì§€ì—°ì‹œê°„: %v\n", eventStats.AverageLatency)

			fmt.Printf("\nğŸ”§ ì‘ì—… í:\n")
			fmt.Printf("   ì´ ì‘ì—…: %d, ì™„ë£Œ: %d, ì‹¤íŒ¨: %d\n",
				queueStats.TotalJobs, queueStats.CompletedJobs, queueStats.FailedJobs)
			fmt.Printf("   í™œì„± ì›Œì»¤: %d, ëŒ€ê¸° ì‘ì—…: %d\n",
				queueStats.ActiveWorkers, queueStats.QueuedJobs)
			fmt.Printf("   ì¬ì‹œë„: %d, í‰ê·  ì‹¤í–‰ì‹œê°„: %v\n",
				queueStats.RetriedJobs, queueStats.AverageExecTime)

			high, normal, low := workQueue.QueueLengths()
			fmt.Printf("   í ê¸¸ì´ - ë†’ìŒ:%d, ë³´í†µ:%d, ë‚®ìŒ:%d\n", high, normal, low)

			// Calculate overall metrics
			totalOps := ioStats.TotalOperations + eventStats.TotalEvents + queueStats.TotalJobs
			if totalOps > 0 && elapsed.Seconds() > 0 {
				throughput := float64(totalOps) / elapsed.Seconds()
				fmt.Printf("\nğŸš€ ì „ì²´ ì²˜ë¦¬ëŸ‰: %.1f ì‘ì—…/ì´ˆ\n", throughput)
			}
		}
	}
}

func generateContinuousWorkload(ctx context.Context, aio *async.AsyncIO, workQueue *async.WorkQueue, eventBus *async.EventBus) {
	tempDir, err := os.MkdirTemp("", "monitor-workload")
	if err != nil {
		return
	}
	defer os.RemoveAll(tempDir)

	jobCounter := 0
	ticker := time.NewTicker(500 * time.Millisecond)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			// Generate file I/O workload
			filename := filepath.Join(tempDir, fmt.Sprintf("workload_%d.txt", jobCounter))
			content := fmt.Sprintf("Workload file %d created at %s", jobCounter, time.Now().Format(time.RFC3339))

			writeResultCh := aio.WriteFileAsync(ctx, filename, []byte(content), 0o644)
			go func() {
				<-writeResultCh
				// Read it back
				readResultCh := aio.ReadFileAsync(ctx, filename)
				<-readResultCh
			}()

			// Generate work queue jobs
			job := async.NewSimpleJob(fmt.Sprintf("workload-%d", jobCounter),
				3+jobCounter%7, // Varying priority
				func(ctx context.Context) error {
					// Simulate work
					work := time.Duration(10+jobCounter%50) * time.Millisecond
					time.Sleep(work)

					// Sometimes fail
					if jobCounter%20 == 0 {
						return fmt.Errorf("simulated failure")
					}
					return nil
				})
			workQueue.Submit(job)

			// Generate events
			event := async.BaseEvent{
				EventType:   "workload.generated",
				EventTime:   time.Now(),
				EventSource: "workload-generator",
				EventData: map[string]interface{}{
					"job_id": jobCounter,
					"type":   "continuous",
				},
			}
			eventBus.PublishAsync(ctx, event)

			jobCounter++
		}
	}
}

func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}
